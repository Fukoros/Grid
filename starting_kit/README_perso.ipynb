{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'example_submission/'\n",
    "problem_dir = 'ingestion_program/'  \n",
    "score_dir = 'scoring_program/'\n",
    "input_dir = 'public_data/'\n",
    "output_dir = 'output/'\n",
    "from sys import path; path.append(model_dir); path.append(problem_dir); path.append(score_dir);\n",
    "path.append(input_dir); path.append(output_dir);\n",
    "%matplotlib inline\n",
    "# Uncomment the next lines to auto-reload libraries (this causes some problem with pickles in Python 3)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import seaborn as sns; sns.set()\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chronics  configuration.yaml  reference_grid.m\r\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'public_data/hard'              # Change this to the directory where you put the input data\n",
    "!ls $data_dir*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom reward signal CustomRewardSignal of file /home/tp-home004/kratovo/MiniProj/Grid/starting_kit/public_data/reward_signal.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ====================================================\n",
      "                     GAME PARAMETERS\n",
      "  ====================================================\n",
      "    max_number_loads_game_over: 6\n",
      "    max_number_prods_game_over: 3\n",
      "    n_timesteps_hard_overflow_is_broken: 10\n",
      "    loadflow_backend: pypower\n",
      "    n_timesteps_consecutive_soft_overflow_breaks: 10\n",
      "    max_seconds_per_timestep: 1.0\n",
      "    n_timesteps_horizon_maintenance: 48\n",
      "    loadflow_mode: AC\n",
      "    hard_overflow_coefficient: 1.0\n",
      "    n_timesteps_soft_overflow_is_broken: 10\n",
      "  ====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pypownet.environment\n",
    "import pypownet.runner\n",
    "data_dir = 'public_data'  \n",
    "environment = pypownet.environment.RunEnv(parameters_folder=os.path.abspath(data_dir),\n",
    "                                              game_level=\"hard\",\n",
    "                                              chronic_looping_mode='natural', start_id=0,\n",
    "                                              game_over_mode=\"soft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using scoring metric: reward\n"
     ]
    }
   ],
   "source": [
    "from scoring_program import libscores\n",
    "from libscores import get_metric\n",
    "metric_name, scoring_function = get_metric()\n",
    "print('Using scoring metric:', metric_name)\n",
    "# Uncomment the next line to display the code of the scoring metric\n",
    "#??scoring_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.935264587402344e-05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAgent(pypownet.agent.Agent):\n",
    "    \"\"\"\n",
    "    An example of a baseline controler that randomly switches the status of one random power line per timestep (if the\n",
    "    random line is previously online, switch it off, otherwise switch it on).\n",
    "    \"\"\"\n",
    "    def __init__(self, environment):\n",
    "        super().__init__(environment)\n",
    "    \n",
    "    def act(self, observation):\n",
    "        \"\"\" Produces an action given an observation of the environment. Takes as argument an observation of the current\n",
    "        power grid, and returns the chosen action.\"\"\"\n",
    "        # Sanity check: an observation is a structured object defined in the environment file.\n",
    "        assert isinstance(observation, pypownet.environment.Observation)\n",
    "        #print(\" DO NOTHING AGENT !!! \")\n",
    "        action_space = self.environment.action_space\n",
    "\n",
    "        # Implement your policy here\n",
    "        # Example of the do-nothing policy that produces no action (i.e. an action that does nothing) each time\n",
    "        do_nothing_action = action_space.get_do_nothing_action()\n",
    "\n",
    "        # Sanity check: verify the good overall structure of the returned action; raises exceptions if not valid\n",
    "        assert action_space.verify_action_shape(do_nothing_action)\n",
    "        return do_nothing_action\n",
    "\n",
    "        # No learning (i.e. self.feed_reward does pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulative rewards : -362.6689729381084\n",
      "12.930731534957886\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import time\n",
    "start = time.time()\n",
    "NUMBER_ITERATIONS = 1000\n",
    "\n",
    "submission_dir = 'example_submission'\n",
    "sys.path.append(submission_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "log_path = os.path.abspath(os.path.join(output_dir, 'runner.log'))\n",
    "\n",
    "\n",
    "open(log_path, 'w').close()\n",
    "submitted_controler = CustomAgent(environment)\n",
    "# Instanciate a runner, that will save the run statistics within the log_path file, to be parsed and processed\n",
    "# by the scoring program\n",
    "phase_runner = pypownet.runner.Runner(environment, submitted_controler, verbose=True, vverbose=False,\n",
    "                                      log_filepath=log_path)\n",
    "phase_runner.ch.setLevel(logging.ERROR)\n",
    "# Run the planned experiment of this phase with the submitted model\n",
    "score = phase_runner.loop(iterations=NUMBER_ITERATIONS)\n",
    "print(\"cumulative rewards : {}\".format(score))\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearch(pypownet.agent.Agent):\n",
    "    \"\"\" This agent is a tree-search model of depth 1, that is constrained to modifiying at most 1 substation\n",
    "    configuration or at most 1 line status. This controler used the simulate method of the environment, by testing\n",
    "    every 1-line status switch action, every new configuration for substations with at least 4 elements, as well as\n",
    "    the do-nothing action. Then, it will seek for the best reward and return the associated action, expecting\n",
    "    the maximum reward for the action pool it can reach.\n",
    "    Note that the simulate method is only an approximation of the step method of the environment, and in three ways:\n",
    "    * simulate uses the DC mode, while step is in AC\n",
    "    * simulate uses only the predictions given to the player to simulate the next timestep injections\n",
    "    * simulate can not compute the hazards that are supposed to come at the next timestep\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, environment):\n",
    "        super().__init__(environment)\n",
    "        self.verbose = False\n",
    "        self.epsilon = 0.1\n",
    "        self.ioman = ActIOnManager(destination_path='saved_actions.csv')\n",
    "        self.ioman2 = ActIOnManager(destination_path='saved_states.csv')\n",
    "        self.ioman3  = ActIOnManager(destination_path='saved_rewards.csv')\n",
    "        random.seed()\n",
    "\n",
    "    def actGS(self, observation):\n",
    "        import itertools\n",
    "\n",
    "         # Sanity check: an observation is a structured object defined in the environment file.\n",
    "        assert isinstance(observation, pypownet.environment.Observation)\n",
    "        action_space = self.environment.action_space\n",
    "\n",
    "        number_lines = action_space.lines_status_subaction_length\n",
    "        # Will store reward, actions, and action name, then eventually pick the maximum reward and retrieve the\n",
    "        # associated values\n",
    "        rewards, actions, names = [], [], []\n",
    "\n",
    "        # Test doing nothing\n",
    "        if self.verbose:\n",
    "            print(' Simulation with no action', end='')\n",
    "        action = action_space.get_do_nothing_action()\n",
    "        reward_aslist = self.environment.simulate(action, do_sum=False)\n",
    "        reward = sum(reward_aslist)\n",
    "        if self.verbose:\n",
    "            print('; reward: [', ', '.join(['%.2f' % c for c in reward_aslist]), '] =', reward)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        names.append('no action')\n",
    "\n",
    "        # Test every line opening\n",
    "        for l in range(number_lines):\n",
    "            if self.verbose:\n",
    "                print(' Simulation with switching status of line %d' % l, end='')\n",
    "            action = action_space.get_do_nothing_action()\n",
    "            action_space.set_lines_status_switch_from_id(action=action, line_id=l, new_switch_value=1)\n",
    "            reward_aslist = self.environment.simulate(action, do_sum=False)\n",
    "            reward = sum(reward_aslist)\n",
    "            if self.verbose:\n",
    "                print('; reward: [', ', '.join(['%.2f' % c for c in reward_aslist]), '] =', reward)\n",
    "            rewards.append(reward)\n",
    "            actions.append(action)\n",
    "            names.append('switching status of line %d' % l)\n",
    "\n",
    "        # For every substation with at least 4 elements, try every possible configuration for the switches\n",
    "        for substation_id in action_space.substations_ids:\n",
    "            substation_n_elements = action_space.get_number_elements_of_substation(substation_id)\n",
    "            if 6 > substation_n_elements > 3:\n",
    "                # Look through all configurations of n_elements binary vector with first value fixed to 0\n",
    "                for configuration in list(itertools.product([0, 1], repeat=substation_n_elements - 1)):\n",
    "                    new_configuration = [0] + list(configuration)\n",
    "                    if self.verbose:\n",
    "                        print(' Simulation with change in topo of sub. %d with switches %s' % (\n",
    "                            substation_id, repr(new_configuration)), end='')\n",
    "                    # Construct action\n",
    "                    action = action_space.get_do_nothing_action()\n",
    "                    action_space.set_switches_configuration_of_substation(action=action,\n",
    "                                                                          substation_id=substation_id,\n",
    "                                                                          new_configuration=new_configuration)\n",
    "                    reward_aslist = self.environment.simulate(action, do_sum=False)\n",
    "                    reward = sum(reward_aslist)\n",
    "                    if self.verbose:\n",
    "                        print('; reward: [', ', '.join(['%.2f' % c for c in reward_aslist]), '] =', reward)\n",
    "                    rewards.append(reward)\n",
    "                    actions.append(action)\n",
    "                    names.append('change in topo of sub. %d with switches %s' % (substation_id,\n",
    "                                                                                 repr(new_configuration)))\n",
    "\n",
    "        # Take the best reward, and retrieve the corresponding action\n",
    "        best_reward = max(rewards)\n",
    "        best_index = rewards.index(best_reward)\n",
    "        best_action = actions[best_index]\n",
    "        best_action_name = names[best_index]\n",
    "\n",
    "        # Dump best action into stored actions file\n",
    "        self.ioman.dump(best_action)\n",
    "        self.ioman3.dumpReward(best_reward)\n",
    "        self.ioman2.dumpState(observation.as_array())\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Action chosen: ', best_action_name, '; expected reward %.4f' % best_reward)\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    def actRNS(self, observation):\n",
    "        # Sanity check: an observation is a structured object defined in the environment file.\n",
    "        assert isinstance(observation, pypownet.environment.Observation)\n",
    "        action_space = self.environment.action_space\n",
    "\n",
    "        # Create template of action with no switch activated (do-nothing action)\n",
    "        action = action_space.get_do_nothing_action()\n",
    "\n",
    "        # Select a random substation ID on which to perform node-splitting\n",
    "        target_substation_id = np.random.choice(action_space.substations_ids)\n",
    "        expected_target_configuration_size = action_space.get_number_elements_of_substation(target_substation_id)\n",
    "        # Choses a new switch configuration (binary array)\n",
    "        target_configuration = np.random.choice([0, 1], size=(expected_target_configuration_size,))\n",
    "\n",
    "        action_space.set_switches_configuration_of_substation(action=action,\n",
    "                                                              substation_id=target_substation_id,\n",
    "                                                              new_configuration=target_configuration)\n",
    "\n",
    "        # Ensure changes have been done on action\n",
    "        current_configuration, _ = action_space.get_switches_configuration_of_substation(action, target_substation_id)\n",
    "        assert np.all(current_configuration == target_configuration)\n",
    "\n",
    "        # Dump best action into stored actions file\n",
    "         #self.ioman.dump(action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def actRLS(self, observation):\n",
    "        # Sanity check: an observation is a structured object defined in the environment file.\n",
    "        assert isinstance(observation, pypownet.environment.Observation)\n",
    "        action_space = self.environment.action_space\n",
    "\n",
    "        # Create template of action with no switch activated (do-nothing action)\n",
    "        action = action_space.get_do_nothing_action()\n",
    "\n",
    "        # Randomly switch one line\n",
    "        l = np.random.randint(action_space.lines_status_subaction_length)\n",
    "        action_space.set_lines_status_switch_from_id(action=action,\n",
    "                                                     line_id=l,\n",
    "                                                     new_switch_value=1)\n",
    "\n",
    "        # Test the reward on the environment\n",
    "        reward_aslist = self.environment.simulate(action, do_sum=False)\n",
    "        reward = sum(reward_aslist)\n",
    "        if self.verbose:\n",
    "            print('reward: [', ', '.join(['%.2f' % c for c in reward_aslist]), '] =', reward)\n",
    "\n",
    "        action_name = 'switching status of line %d' % l\n",
    "        if self.verbose:\n",
    "            print('Action chosen: ', action_name, '; expected reward %.4f' % reward)\n",
    "\n",
    "        return action\n",
    "\n",
    "        # No learning (i.e. self.feed_reward does pass)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def act(self, observation):\n",
    "        x = random.random()\n",
    "        if x<= self.epsilon:\n",
    "            return self.actRNS(observation)\n",
    "        elif x <= 2*self.epsilon:\n",
    "            return self.actRLS(observation)\n",
    "        else:\n",
    "            return self.actGS(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import example_submission.preprocessing as pp\n",
    "prepro = pp.Preprocessing(\"saved_actions.csv\",\"saved_states.csv\",\"saved_rewards.csv\")\n",
    "actionDict = prepro.reduce_actions()\n",
    "actionList = list(actionDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "intToAct = dict()\n",
    "for i in range(len(actionList)):\n",
    "    intToAct[i] = actionList[i]\n",
    "\n",
    "actToInt = dict()\n",
    "for i in range(len(actionList)):\n",
    "    actToInt[actionList[i]] = i\n",
    "\n",
    "def codeAction(action):\n",
    "    return actToInt[action]\n",
    "\n",
    "def decodeAction(key):\n",
    "    return intToAct[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"action_count = np.zeros(len(actionList))\n",
    "\n",
    "with open(\"saved_actions.csv\",'r') as doneActions:\n",
    "    lineN = 0\n",
    "    while lineN < 10001:\n",
    "        action = doneActions.readline()\n",
    "        action = action.replace(\",\",\"\")\n",
    "        action = action.replace(\"\\n\",\"\")\n",
    "        #print(\"wazzup\")\n",
    "        if lineN == 0:\n",
    "            print(action)\n",
    "            print(actionList[12])\n",
    "        for i in range(len(actionList)):\n",
    "            #print(action is actionList[i])\n",
    "            if actionList[i] == action:\n",
    "                action_count[i] += 1\n",
    "        lineN += 1\n",
    "\n",
    "print(action_count)\"\"\"\n",
    "histArray = []\n",
    "with open(\"saved_actions.csv\",'r') as doneActions:\n",
    "    lineN = 0\n",
    "    while lineN < 10001:\n",
    "        action = doneActions.readline()\n",
    "        action = action.replace(\",\",\"\")\n",
    "        action = action.replace(\"\\n\",\"\")\n",
    "        histArray.append(codeAction(action))\n",
    "        lineN += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.600e+01, 2.000e+00, 1.320e+02, 1.450e+02, 7.000e+01, 3.900e+01,\n",
       "        9.650e+02, 4.000e+00, 1.000e+00, 3.000e+00, 2.000e+00, 6.100e+01,\n",
       "        1.500e+02, 1.000e+00, 3.780e+02, 6.400e+01, 3.300e+01, 1.000e+02,\n",
       "        2.071e+03, 1.120e+02, 1.700e+01, 1.800e+02, 1.148e+03, 2.000e+00,\n",
       "        9.700e+01, 0.000e+00, 6.700e+01, 4.100e+01, 1.230e+02, 1.900e+01,\n",
       "        1.000e+00, 1.220e+02, 6.100e+01, 0.000e+00, 1.870e+02, 2.000e+00,\n",
       "        4.000e+00, 1.160e+02, 3.000e+00, 4.000e+00, 2.000e+00, 4.770e+02,\n",
       "        1.710e+02, 2.900e+01, 1.400e+01, 1.900e+01, 1.600e+01, 1.200e+01,\n",
       "        0.000e+00, 1.000e+00, 7.700e+01, 2.284e+03, 3.360e+02]),\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "        51, 52, 53]),\n",
       " <a list of 53 Patch objects>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(histArray,np.arange(54))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
